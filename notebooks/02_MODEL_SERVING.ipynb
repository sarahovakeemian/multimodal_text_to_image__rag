{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87624fcb-bed4-4e54-9923-960cd9ed31fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from config import DeployConfig\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb1ba28-70d7-458c-be9b-7efc9b88047b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(mlflow.__version__)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa667db6-abf6-4429-a841-8076fac4fc54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"config_path\", \"./config/env_variables.yml\")\n",
    "config_path = dbutils.widgets.get(\"config_path\")\n",
    "cfg = DeployConfig.from_yaml(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ba483c-3969-4b7c-81e2-e322801969cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_table = getattr(cfg, f\"image_table\")\n",
    "dev_model = getattr(cfg, f\"dev_model\")\n",
    "endpoint_name = getattr(cfg, f\"endpoint_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67ed1666-eb36-42af-ba7f-9dc9f2748085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PYFUNC MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c50f29-9738-4dfe-88ff-b76b5c5cbd64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# JUST IMAGE EMBEDDING\n",
    "class CLIP_IMAGE_EMBEDDING(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        # Initialize tokenizer and model\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.processor= CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    \n",
    "    def _get_image_embedding_bytearray(self, base64_image_str):\n",
    "        import base64\n",
    "        from PIL import Image\n",
    "        import requests\n",
    "        from io import BytesIO\n",
    "        #decode base64string back to bytearray\n",
    "        decoded_bytearray = bytearray(base64.b64decode(base64_image_str))\n",
    "        image = Image.open(BytesIO(decoded_bytearray))\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        image_features = self.model.get_image_features(**inputs)\n",
    "        return image_features.detach().numpy().tolist()[0]\n",
    "\n",
    "    def predict(self, context, df):\n",
    "        return df['model_input'].apply(lambda x: self._get_image_embedding_bytearray(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255e53d7-925a-4a5a-8b1d-36cab7ae8764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# IMAGE AND TEXT EMBEDDING MODEL\n",
    "class CLIP_IMAGE_TEXT_EMBEDDING(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        # Initialize tokenizer and model\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.processor= CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    \n",
    "    def _get_image_embedding_bytearray(self, base64_image_str):\n",
    "        from PIL import Image\n",
    "        import requests       \n",
    "        from io import BytesIO\n",
    "        import base64\n",
    "        #decode base64string back to bytearray\n",
    "        decoded_bytearray = bytearray(base64.b64decode(base64_image_str))\n",
    "        image = Image.open(BytesIO(decoded_bytearray))\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        image_features = self.model.get_image_features(**inputs)\n",
    "        return image_features.detach().numpy().tolist()[0]\n",
    "\n",
    "    def _get_text_embedding(self, text):\n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "        text_features = self.model.get_text_features(**inputs)\n",
    "        return text_features.detach().numpy().tolist()[0]\n",
    "\n",
    "    def predict(self, context, df, params):\n",
    "        input_type=params.get('input_type')\n",
    "        if input_type.lower()=='image':\n",
    "          print('embedding image')\n",
    "          return df['model_input'].apply(lambda x: self._get_image_embedding_bytearray(x))\n",
    "        elif input_type.lower()=='text':\n",
    "          print('embedding text')\n",
    "          return df['model_input'].apply(lambda x: self._get_text_embedding(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d3bcbc6-bc42-4e03-b47e-29509f3ae13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# TESTING PYFUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e84eec-c453-41bc-9b54-e76dea97f97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_test_pd=spark.sql(f'select model_input from {image_table.path} limit 2').toPandas()\n",
    "image_test_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93fa0683-31b1-4eda-877c-4dbc1ee4ca8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clip=CLIP_IMAGE_EMBEDDING()\n",
    "clip.load_context(context=None)\n",
    "test_result=clip.predict(context=None, df=image_test_pd)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe80679-7ce6-4cc1-9905-2b9d2c2cd747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "clip=CLIP_IMAGE_TEXT_EMBEDDING()\n",
    "clip.load_context(context=None)\n",
    "test_result=clip.predict(context=None, df=image_test_pd, params={'input_type':'image'})\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d888df-b4dc-496d-a5f9-1aecbee5e52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_text_input=pd.DataFrame({'model_input':['hello world', 'hello world']})\n",
    "test_result=clip.predict(context=None, df=df_text_input, params={'input_type':'text'})\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf14986-8c9a-4b3d-91e2-a8190295eaf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "#check out image\n",
    "test_image=spark.sql(f'select content from {image_table.path} limit 1').collect()[0]['content']\n",
    "Image.open(BytesIO(test_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff492aac-5d5f-4bf0-9b88-2ab477048b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LOG MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f5af0b-e72d-4c9c-a1ae-7d399e7378e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip_requirements=[\n",
    "  \"--extra-index-url https://download.pytorch.org/whl/cu121\", \n",
    "  \"mlflow==2.15.1\",\n",
    "  \"setuptools<70.0.0\", \n",
    "  \"torch==2.3.1+cu121\", \n",
    "  \"accelerate==0.31.0\", \n",
    "  \"astunparse==1.6.3\", \n",
    "  \"bcrypt==3.2.0\", \n",
    "  \"boto3==1.34.39\", \n",
    "  \"configparser==5.2.0\", \n",
    "  \"defusedxml==0.7.1\", \n",
    "  \"dill==0.3.6\",\n",
    "   \"google-cloud-storage==2.10.0\", \n",
    "   \"ipython==8.15.0\", \n",
    "   \"lz4==4.3.2\", \n",
    "   \"nvidia-ml-py==12.555.43\", \n",
    "   \"optree==0.12.1\", \n",
    "   \"pandas==1.5.3\", \n",
    "   \"pyopenssl==23.2.0\", \n",
    "   \"pytesseract==0.3.10\", \n",
    "   \"scikit-learn==1.3.0\", \n",
    "   \"sentencepiece==0.1.99\", \n",
    "   \"torchvision==0.18.1+cu121\", \n",
    "   \"transformers==4.41.2\",\n",
    "   \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.3cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\"\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c17044-72b4-4092-9fce-d5b588315cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature, infer_signature\n",
    "\n",
    "signature1 = infer_signature(image_test_pd, [test_result[0]])\n",
    "signature2 = infer_signature(image_test_pd, [test_result[0]], params={'input_type':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02982752-5fda-4c43-ac26-e3a149e640e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/Users/sara.hovakeemian@databricks.com/FE-IP/experiments/clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c30d615-a068-4efb-a851-d023840fa08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#image only embedding model\n",
    "with mlflow.start_run(run_name='clip_image_only') as run:  \n",
    "    mlflow.pyfunc.log_model(\n",
    "        registered_model_name=f'{dev_model.path}',\n",
    "        python_model=CLIP_IMAGE_EMBEDDING(),\n",
    "        artifact_path=\"clip_image_only\",\n",
    "        signature=signature1,\n",
    "        pip_requirements=pip_requirements\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e4abb4-442f-4d38-8e80-49d048c53e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f6a380-bad7-42a4-bae3-656b53481100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.set_registered_model_alias(name=f'{dev_model.path}', alias='champion', version=1)\n",
    "\n",
    "client.update_model_version(\n",
    "    name=f'{dev_model.path}',\n",
    "    version=1,\n",
    "    description=\"Only does image embeddings using CLIP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d15ad5-15a8-4547-938c-f720e73ad131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#image only embedding model\n",
    "with mlflow.start_run(run_name='clip_image_text') as run:  \n",
    "    mlflow.pyfunc.log_model(\n",
    "        registered_model_name=f'{dev_model.path}',\n",
    "        python_model=CLIP_IMAGE_TEXT_EMBEDDING(),\n",
    "        artifact_path=\"clip_image_text\",\n",
    "        signature=signature2,\n",
    "        pip_requirements=pip_requirements\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de48f67c-7017-4596-b47f-a6d1b304b6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.set_registered_model_alias(name=f'{dev_model.path}', alias='challenger', version=2)\n",
    "\n",
    "client.update_model_version(\n",
    "    name=f'{dev_model.path}',\n",
    "    version=2,\n",
    "    description=\"Can do image and text embeddings using CLIP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05224f3c-74f7-40a4-b877-d13b68342d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SERVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e506232f-dabd-461c-a2f7-e49c6f0c1f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Define the endpoint URL and headers\n",
    "url = \"https://e2-demo-field-eng.cloud.databricks.com/api/2.0/serving-endpoints\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {notebook_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the payload for creating the model serving endpoint\n",
    "payload = {\n",
    "    \"name\": endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"entity_name\": f\"{dev_model.path}\",\n",
    "                \"entity_version\": 1,\n",
    "                \"workload_size\": \"Medium\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"workload_type\": \"GPU_SMALL\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make the POST request to create the serving endpoint\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "# Check the response status\n",
    "if response.status_code == 200:\n",
    "    print(\"Model serving endpoint created successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to create model serving endpoint: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e82dd2ec-feb7-4451-8224-1661670a5be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# QUERY ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a9cbc3-c2d5-4c1f-a6f9-3c324a9376fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_table.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e87844-5e28-42c3-a025-722b08cbfd39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_test_pd=spark.sql(f'select model_input from {image_table.path} limit 1').toPandas()\n",
    "image_base_64=image_test_pd.head(1).iloc[0]['model_input']\n",
    "\n",
    "\n",
    "# Define the model serving endpoint URL\n",
    "endpoint_url = f\"https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/{endpoint_name}/invocations\"\n",
    "\n",
    "input_data = {\n",
    "  \"inputs\" : [image_base_64]\n",
    "  # ,\"params\" : {'input_type':'image'} #use if using the model that can produce text and image embeddings\n",
    "}\n",
    "\n",
    "notebook_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Set the headers for the request\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f'Bearer {notebook_token}'\n",
    "}\n",
    "\n",
    "# Make the request to the model serving endpoint\n",
    "response = requests.post(endpoint_url, headers=headers, data=json.dumps(input_data))\n",
    "\n",
    "# Parse the response\n",
    "response_data = response.json()\n",
    "\n",
    "# Display the response data\n",
    "display(response_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45e0e87d-c5f5-4531-bb71-99e915dcd521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_MODEL_SERVING",
   "widgets": {
    "config_path": {
     "currentValue": "./config/env_variables.yml",
     "nuid": "3fb3e4dd-aba7-4ffe-a139-4670ee62ae8f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "./config/env_variables.yml",
      "label": null,
      "name": "config_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "./config/env_variables.yml",
      "label": null,
      "name": "config_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
